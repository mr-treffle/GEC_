{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "\n",
    "import os\n",
    "import sys,re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, GlobalMaxPool1D, Bidirectional, Conv1D\n",
    "from tensorflow.keras.layers import LSTM, MaxPooling1D, Embedding, Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load in pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "word2vec = {}\n",
    "with open(os.path.join('/home/hassan/Documents/machine_learning_examples/nlp_class3/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare text samples and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "found line 57151\n"
     ]
    }
   ],
   "source": [
    "sentences = np.empty(0,dtype='object')\n",
    "targets = np.zeros((0,28))\n",
    "labels = {\"Vt\":0,\"Vm\":1,\"V0\":2,\"Vform\":3,\"SVA\":4,\"ArtOrDet\":5,\"Nn\":6,\"Npos\":7,\"Pform\":8,\"Pref\":9,\"Prep\":10,\"Wci\":11,\"Wa\":12,\"Wform\":13,\"Wtone\":14,\"Srun\":15,\"Smod\":16,\"Spar\":17,\"Sfrag\":18,\"Ssub\":19,\"WOinc\":20,\"WOadv\":21,\"Trans\":22,\"Mec\":23,\"Rloc\":24,\"Cit\":25,\"Others\":26,\"Um\":27}\n",
    "i=0\n",
    "f=open('/home/hassan/Documents/machine_learning_examples/nlp_class3/toxic/conll14st-preprocessed.m2')\n",
    "line = f.readline()\n",
    "while line:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        # print(line)\n",
    "        if re.search(\"^S\",line):\n",
    "            a=np.empty(1,dtype='object')\n",
    "            a[0]=line[2:]\n",
    "            sentences=np.concatenate([sentences,a])\n",
    "            b=np.zeros((1,28))\n",
    "            targets=np.concatenate([targets,b])\n",
    "            i=i+1\n",
    "        elif re.search(\"^A\",line):\n",
    "            # print(\"found annotation\")\n",
    "            for key in labels:\n",
    "                # print(labels.get(key))\n",
    "                if re.search(key,line):\n",
    "                    # print(labels.get(key))\n",
    "                    targets[i-1,labels.get(key)]=1\n",
    "                    break\n",
    "    line=f.readline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0.]\n",
      "found line 57151\n"
     ]
    }
   ],
   "source": [
    "print(targets[3])\n",
    "print(targets[5])\n",
    "print(\"found line\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the sentences (strings) into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization...\n"
     ]
    }
   ],
   "source": [
    "print('Tokenization...')\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "word2idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence[0]:  [1113, 7, 7661, 228]\n",
      "max sequence length: 131\n",
      "min sequence length: 0\n",
      "Found 25893 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"sequence[0]: \", sequences[0])\n",
    "\n",
    "print(\"max sequence length:\", max(len(s) for s in sequences))\n",
    "print(\"min sequence length:\", min(len(s) for s in sequences))\n",
    "# s = sorted(len(s) for s in sequences)\n",
    "# print(\"median sequence length:\", s[len(s) // 2])\n",
    "#\n",
    "# print(\"max word index:\", max(max(seq) for seq in sequences if len(seq) > 0))\n",
    "\n",
    "print('Found %s unique tokens.' % len(word2idx))\n",
    "# print(next(iter(word2idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad sequences so that we get a N x T matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences...\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences...')\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0] [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0 1113    7\n",
      " 7661  228]\n",
      "Shape of data tensor: (57151, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"data[0]\", data[0])\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n",
      "len word2idx 25893\n"
     ]
    }
   ],
   "source": [
    "print('Filling pre-trained embeddings...')\n",
    "print(\"len word2idx\", len(word2idx))\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "  if i < MAX_VOCAB_SIZE:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "      embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix shape: (20000, 50)\n",
      "1113\n",
      "embedding_matrix[1113] [ 0.66447997  0.059328   -0.10265     0.072485   -0.042958    0.62238002\n",
      " -0.061768   -0.76239997  0.18791001  0.50511998 -0.18424     0.010916\n",
      " -0.21409     0.29516    -0.18311     0.080493    0.69437999  0.26245001\n",
      "  0.48195001 -0.66535997  0.67246997 -0.20181    -0.47898    -0.079556\n",
      "  0.085443   -0.67527997 -0.38523999 -0.027527    0.80712998  0.10879\n",
      "  3.27469993  0.16201    -0.21815    -1.19159997 -0.70274001  0.60671997\n",
      " -0.65700001  0.065899   -0.41716999 -0.25497001 -0.26298001 -0.25551\n",
      " -0.065865    0.08621    -0.28215     0.32980999  0.075665    0.013673\n",
      " -0.16734999 -0.17380001]\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding_matrix shape:\", embedding_matrix.shape)\n",
    "print(word2idx.get(\"creating\"))\n",
    "print(\"embedding_matrix[1113]\", embedding_matrix[1113])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load pre-trained word embeddings into an Embedding layer <br/>\n",
    "note that we set trainable = False so as to keep the embeddings fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Embedding layer ...\n"
     ]
    }
   ],
   "source": [
    "print('Building Embedding layer ...')\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=MAX_SEQUENCE_LENGTH,\n",
    "  trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x = embedding_layer(input_)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(28, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(input_, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "   loss='binary_crossentropy',\n",
    "   optimizer='rmsprop',\n",
    "   metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  data,\n",
    "  targets,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=VALIDATION_SPLIT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(r.history['accuracy'], label='acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(data)\n",
    "aucs = []\n",
    "for j in range(6):\n",
    "    auc = roc_auc_score(targets[:,j], p[:,j])\n",
    "    aucs.append(auc)\n",
    "print(np.mean(aucs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
